{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final Análisis Aplicado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equipo:\n",
    "\n",
    "Bernardo Paniagua 158372.\n",
    "\n",
    "Christian Rodríguez 164918.\n",
    "\n",
    "David Martín del Campo 158546.\n",
    "\n",
    "Zara Ubaldo 166583."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo se presenta la clase de python \"optimizacion\" desarrollada por el equipo. La clase pide como argumentos una función, una cantidad de iteraciones y una tolerancia. Como funciones de clase, puede calcular el gradiente de la función, la matriz hessiana y buscar un óptimo por medio de los metodos de Newton, Busqueda Lineal de Newton, Busqueda Lineal de Newton con Modificaciones a la Hessiana y BFGS.\n",
    "\n",
    "Después de la clase, se presentan las dos funciones en las que se se pusieron a prueba los algoritmos. Una es la función de Rosenbrok, otra es un problema de colocación.\n",
    "\n",
    "Por último, hay un algoritmo para la visualización del problema de colocación.\n",
    "\n",
    "En el archivo optimizacion.py junto a este archivo, se encuentra este mismo programa con tipo de documento .py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizacion:\n",
    "    def __init__(self,function,max_iters=-1,tol=1):\n",
    "        self.f = function\n",
    "        self.max_iters=300 if max_iters==-1 else max_iters\n",
    "        self.tol=np.float64(1e-10) * np.ones(np.size(x)) if tol==1 else tol\n",
    "\n",
    "    def gradiente(self,x):\n",
    "        x = x.astype(np.float64)\n",
    "        h = np.float64(1e-4)\n",
    "        k = 1/(2*h)\n",
    "        n = x.shape[0]\n",
    "        grad = np.zeros(n).astype(np.float64)\n",
    "        for i in range(n):\n",
    "            aux1 = np.copy(x)\n",
    "            aux2 = np.copy(x)\n",
    "            aux1[i] = aux1[i]+h\n",
    "            aux2[i] = aux2[i]-h\n",
    "            grad[i] = self.f(aux1) - self.f(aux2)\n",
    "            grad[i] = grad[i]*k\n",
    "        return grad\n",
    "    \n",
    "    def hessiana(self,x):\n",
    "        x = x.astype(np.float64)\n",
    "        h = np.float64(1e-2)\n",
    "        k = 1/(h**2)\n",
    "        n = x.shape[0]\n",
    "        hess = np.zeros((n,n)).astype(np.float64)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1):\n",
    "                if i == j:\n",
    "                    aux1 = np.copy(x)\n",
    "                    aux2 = np.copy(x)\n",
    "                    aux1[i] = aux1[i]+h\n",
    "                    aux2[i] = aux2[i]-h\n",
    "                    hess[i,j] = self.f(aux1) + self.f(aux2) - 2*self.f(x)\n",
    "                    hess[i,j] = hess[i,j]*k\n",
    "                else:\n",
    "                    aux1 = np.copy(x)\n",
    "                    aux2 = np.copy(x)\n",
    "                    aux3 = np.copy(x)\n",
    "                    aux1[i] = aux1[i]+h\n",
    "                    aux1[j] = aux1[j]+h\n",
    "                    aux2[i] = aux2[i]+h\n",
    "                    aux3[j] = aux3[j]+h\n",
    "                    hess[i,j] = self.f(aux1) - self.f(aux2) - self.f(aux3) + self.f(x)\n",
    "                    hess[i,j] = hess[i,j]*k\n",
    "                    hess[j,i] = hess[i,j]\n",
    "        return hess\n",
    "    \n",
    "    def es_optimo(self,x):\n",
    "        grad = abs(self.gradiente(x))\n",
    "        if all(grad < self.tol):\n",
    "            hess = self.hessiana(x)\n",
    "            if all (np.linalg.eigh(hess)[0] >= 0):\n",
    "                optimo = True\n",
    "            else:\n",
    "                optimo = False\n",
    "        else:\n",
    "            optimo = False\n",
    "        return optimo\n",
    "    \n",
    "    def wolfe(self,x,p,alpha,c1,c2):\n",
    "        aux = x + alpha*p\n",
    "        producto = np.dot(self.gradiente(x),p)\n",
    "        cond1=False\n",
    "        cond2=False\n",
    "        if self.f(aux) <= self.f(x) + c1*alpha*producto:\n",
    "            cond1=True\n",
    "        if np.dot(self.gradiente(aux),p) >= c2*producto:\n",
    "            cond2=True\n",
    "        return cond1 and cond2\n",
    "    \n",
    "    def alpha(self,x,p,c1,c2,rho):\n",
    "        a = 1\n",
    "        i = 1\n",
    "        M = 500\n",
    "        while i<M and self.wolfe(x,p,a,c1,c2)==False:\n",
    "            a = rho*a\n",
    "            i = i+1\n",
    "        if i==M:\n",
    "            print(\"iteraciones maximas alcanzadas en Wolfe\")\n",
    "        return a\n",
    "    \n",
    "    def volver_pd(self,x):\n",
    "        if np.all(np.linalg.eigvals(x) > 0) == True:\n",
    "            return x\n",
    "        else:\n",
    "            e = abs(np.linalg.eigvals(x))\n",
    "            l = min(e) + 3*np.finfo(float).eps\n",
    "            E = np.identity(len(x))\n",
    "            x = x+(l*E)\n",
    "        return x\n",
    "    \n",
    "    def busqueda_lineal_newton(self,x,modificada = False,comparacion=False):\n",
    "        k=0\n",
    "        c1 = 0.0001\n",
    "        c2 = 0.9\n",
    "        rho = 0.6\n",
    "        a = 1\n",
    "        while k<self.max_iters and self.es_optimo(x)==False:\n",
    "            B = self.hessiana(x)\n",
    "            g = -self.gradiente(x)\n",
    "            p = np.linalg.solve(B,g)\n",
    "            if modificada == True:\n",
    "                B = self.volver_pd(B)\n",
    "                a = self.alpha(x,p,c1,c2,rho)\n",
    "            x = x+a*p\n",
    "            k=k+1\n",
    "        if k==self.max_iters:\n",
    "            if modificada:\n",
    "                print (\"iteraciones maximas alcanzadas en busqueda lineal modificada\")\n",
    "            else:\n",
    "                print (\"iteraciones maximas alcanzadas en busqueda lineal\")\n",
    "        elif comparacion:\n",
    "            if modificada:\n",
    "                print(\"Busqueda Lineal Newton modificada tardó \",k,\" iteraciones en llegar al óptimo\")\n",
    "            else:\n",
    "                print(\"Busqueda Lineal Newton tardó \",k,\" iteraciones en llegar al óptimo\")\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def BFGS (self,xk,comparacion=False):\n",
    "        I=np.identity(np.size(xk))\n",
    "        H=I\n",
    "        c1 = 10**(-4)\n",
    "        c2 = 0.9\n",
    "        rho = 0.6\n",
    "        k=0\n",
    "        while k<self.max_iters and self.es_optimo(xk)==False:\n",
    "            #H=self.volver_pd(H)\n",
    "            pk=np.linalg.lstsq(-H,self.gradiente(xk),rcond=None)\n",
    "            pk=pk[0]\n",
    "            if k==0:\n",
    "                a=1\n",
    "            else:\n",
    "                a=self.alpha(xk, pk, c1, c2, rho)\n",
    "            xk1=xk+a*pk\n",
    "            sk=xk1-xk\n",
    "            yk=self.gradiente(xk1)-self.gradiente(xk)\n",
    "            rho_k=1/(np.dot(yk,sk))\n",
    "            A=(I-(rho_k*np.dot(sk,yk)))\n",
    "            B=(I-(rho_k*np.dot(yk,sk)))\n",
    "            C=rho_k*np.dot(sk,sk)\n",
    "            #H=(I-(rho_k*np.dot(sk,yk)))*H*(I-(rho_k*np.dot(yk,sk)))+rho_k*np.dot(sk,sk)\n",
    "            H=np.matmul(A,H)\n",
    "            H=np.matmul(H,B)\n",
    "            H=H+C\n",
    "            xk=xk1\n",
    "            k=k+1\n",
    "        if k==self.max_iters:\n",
    "            print (\"iteraciones máximas alcanzadas en BFGS\")\n",
    "        elif comparacion:\n",
    "            print(\"BFGS tardó \",k,\" iteraciones en llegar al óptimo\")\n",
    "        return xk\n",
    "    \n",
    "    def algoritmo_newton(self,x,comparacion=False):\n",
    "        k=0\n",
    "        c1 = 0.1\n",
    "        c2 = 0.8\n",
    "        rho = 0.9\n",
    "        a = 1\n",
    "        while k<self.max_iters and self.es_optimo(x)==False:\n",
    "            B = self.hessiana(x)\n",
    "            g = -self.gradiente(x)\n",
    "            p = np.linalg.solve(B,g)\n",
    "            a = self.alpha(x,p,c1,c2,rho)\n",
    "            x = x+a*p\n",
    "            k=k+1\n",
    "        if k==self.max_iters:\n",
    "            print (\"iteraciones máximas alcanzadas en Newton\")\n",
    "        elif comparacion:\n",
    "            print(\"Newton tardó \",k,\" iteraciones en llegar al óptimo\")\n",
    "        return x\n",
    "    \n",
    "    def comparacion(self,x):\n",
    "        start_time = time.time()\n",
    "        x_N=self.algoritmo_newton(x,True)\n",
    "        f_N=self.f(x_N)\n",
    "        print(\"Mínimo de Newton: \",x_N,\" y es: \",f_N)\n",
    "        t_N=(time.time() - start_time)\n",
    "        print(\"Tardó \",t_N)\n",
    "        start_time = time.time()\n",
    "        x_bln=self.busqueda_lineal_newton(x,modificada=False,comparacion=True)\n",
    "        f_bln=self.f(x_bln)\n",
    "        print(\"Mínimo de búsqueda lineal de Newton: \",x_bln,\", y es: \",f_bln)\n",
    "        t_BLN=(time.time() - start_time)\n",
    "        print(\"Tardó \",t_BLN)\n",
    "        start_time = time.time()\n",
    "        x_blnm=self.busqueda_lineal_newton(x,modificada=True,comparacion=True)\n",
    "        f_blnm=self.f(x_blnm)\n",
    "        print(\"Mínimo de búsqueda lineal de Newton modificada: \",x_blnm,\", y es: \",f_blnm)\n",
    "        t_BLNM=(time.time() - start_time)\n",
    "        print(\"Tardó \",t_BLNM)\n",
    "        start_time = time.time()\n",
    "        x_BFGS=self.BFGS(x,True)\n",
    "        f_BFGS=self.f(x_BFGS)\n",
    "        print(\"Mínimo de BFGS: \",x_BFGS,\" y es: \",f_BFGS)\n",
    "        t_BFGS=(time.time() - start_time)\n",
    "        print(\"Tardó \",t_BFGS)\n",
    "        \n",
    "        y=np.array([t_N,t_BLN,t_BLNM,t_BFGS])\n",
    "        x=np.array([1,2,3,4])\n",
    "        #y = np.array([0.650, 0.660, 0.675, 0.685])\n",
    "        my_xticks = ['Newton', 'BLN', 'BLNM', 'BFGS']\n",
    "        plt.xticks(x, my_xticks)\n",
    "        #plt.yticks(np.arange(y.min(), y.max(), 0.005))\n",
    "        plt.plot(x, y)\n",
    "        plt.grid(axis='y', linestyle='-')\n",
    "        plt.title(\"Tiempos\")\n",
    "        plt.ylabel(\"tiempo en segundos\")\n",
    "        plt.xlabel(\"Métodos\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de Rosenbrock\n",
    "\n",
    " a=1\n",
    " b=100    \n",
    " fR = lambda x: (a-x[0])**2 + b*(x[1]-x[0]**2)**2\n",
    " x = np.array([0,0])\n",
    " optR=optimizacion(fR,max_iters=1000)\n",
    " optR.comparacion(x)\n",
    "\n",
    "# Prueba con diversos parámetros\n",
    "\n",
    " for i in range(1,11,4):\n",
    "     print(\"para a=\",i)\n",
    "     a=i\n",
    "     b=100    \n",
    "     fR = lambda x: (a-x[0])**2 + b*(x[1]-x[0]**2)**2\n",
    "     x = np.array([0,0])\n",
    "     optR=optimizacion(fR,max_iters=100)\n",
    "     optR.comparacion(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problema cámaras\n",
    "\n",
    "datos=np.loadtxt(open(\"crime_data.txt\", \"rb\"), delimiter=\",\", skiprows=1,usecols=range(3,5))\n",
    "n=800\n",
    "x = np.ones(2*n)\n",
    "for i in range(0,2*n):\n",
    "    x[i]=x[i]*random.uniform(1,20)\n",
    "def fCrimen(x):\n",
    "    x = x.reshape(2,n)\n",
    "    r=0\n",
    "    for i in range(0,n):\n",
    "        for j in range(0,n):\n",
    "            r=r+(np.linalg.norm(x[:,i]-datos[j]))**2\n",
    "    for ii in range(0,n):\n",
    "        for jj in range(0,n):\n",
    "            if ii != jj:\n",
    "                r=r+1/((np.linalg.norm(x[:,ii]-x[:,jj]))**2)\n",
    "    return r\n",
    "\n",
    "print(\"Valor inicial: \",fCrimen(x))\n",
    "optC=optimizacion(fCrimen,max_iters=2)\n",
    "r=optC.BFGS(x)\n",
    "print(\"El valor mínimo obtenido fue de \",fCrimen(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de la solucion\n",
    "\n",
    "import folium\n",
    "\n",
    "map = folium.Map(location=[40,-95], zoom_start = 4)\n",
    "\n",
    "points = []\n",
    "for city in tour:\n",
    "  points.append(coordinates[city])\n",
    "points.append(points[0])\n",
    "\n",
    "folium.PolyLine(points).add_to(map)\n",
    "\n",
    "map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
